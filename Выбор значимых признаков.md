parent: [[Построение хороших обучающих наборов - предварительная обработка данных]]

tags: #ml #mlbookclub #reading 

Если мы замечаем, что модель работает гораздо лучше на обучающем наборе данных, чем на испытательном наборе данных, то такое наблюдение является явным сигналом [[Переобучение (Overfiting)|переобучения]].

Переобучение означает, что модель слишком сильно подгоняется к параметрам касаемо отдельных наблюдений в обучающем наборе данных, но плохо обобщается на новые данные. Также говорят, что модель имеет высокую [[Дисперсия (Variance)|дисперсию]]. Причина переобучения состоит в том, что модель является слишком сложной для имеющихся обучающих данных.

Общепринятые решения, направленные на сокращение ошибки обобщения:

- Накопление большего количества обучающих данных
- Введение штрафа за сложность через регуляризацию
- Выбор более простой модели с меньшим числом параметров
- Понижение размерности данных