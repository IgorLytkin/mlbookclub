parent: [[Обзор классификаторов на основе ML с использованием scikit-learn]]

tags: #ml #mlbookclub #reading #scikitlearn #logistic_regression

Главный недостаток персептрона в том, что он никогда не [[Сходимость персептронов|сходится]] для классов, которые нельзя разделить линейно.

Рассмотрим еще один простой, но мощный алгоритм для двоичной классификации - логистическую регрессию. Не смотря на название, алгоритм логистической регрессии является алгоритмом классификации, а не регрессии.

Не смотря на то, что логистическая регрессия применяется для двоичной классификации, ее легко обобщить и для нескольких классов (multinominal и softmax), а также с помощью методики [[Методика OvA для многоклассовой классификации|OvA]], как и для персептрона.

Главная идея, лежащая в основе логистической регрессии как вероятностного классификатора, вводят такой термин как перевес (odds) - это смещение в сторону определенного события. Вычисляется как:
$$p=\frac{p}{1-p}$$
где $p$ - вероятность положительного исхода (события).

Так называемая логит-функция (logit) вычисляется как логарифм перевеса:
$$logit(p)=\ln\frac{p}{1-p}$$
Логит-функция принимает входные значения в диапазоне от 0 до 1 и трансформирует их в значения по всему диапазону вещественных чисел
$$logit(p)=w_1x_1+\dots+w_mx_m+b=\sum_{i=j}w_jx_j+b=w^Tx+b$$
здесь $p$ - вероятность того, что образец принадлежит к определенному классу. Фактически нас интересует прогнозирование вероятности того, что образец принадлежит к конкретному классу, а это является обратной формой логит-функции. Ее еще называют сигмоидом из-за характерной S-образной формы:
$$\sigma(z)=\frac{1}{1-e^{-z}}$$
здесь $z$ - общий вход
$$z=w^Tx+b$$
